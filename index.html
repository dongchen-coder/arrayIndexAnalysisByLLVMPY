<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<style>
*{margin:0;padding:0;}
body {
	font:13.34px helvetica,arial,freesans,clean,sans-serif;
	color:black;
	line-height:1.4em;
	background-color: #F8F8F8;
	padding: 0.7em;
}
p {
	margin:1em 0;
	line-height:1.5em;
}
table {
	font-size:inherit;
	font:100%;
	margin:1em;
}
table th{border-bottom:1px solid #bbb;padding:.2em 1em;}
table td{border-bottom:1px solid #ddd;padding:.2em 1em;}
input[type=text],input[type=password],input[type=image],textarea{font:99% helvetica,arial,freesans,sans-serif;}
select,option{padding:0 .25em;}
optgroup{margin-top:.5em;}
pre,code{font:12px Monaco,"Courier New","DejaVu Sans Mono","Bitstream Vera Sans Mono",monospace;}
pre {
	margin:1em 0;
	font-size:12px;
	background-color:#eee;
	border:1px solid #ddd;
	padding:5px;
	line-height:1.5em;
	color:#444;
	overflow:auto;
	-webkit-box-shadow:rgba(0,0,0,0.07) 0 1px 2px inset;
	-webkit-border-radius:3px;
	-moz-border-radius:3px;border-radius:3px;
}
pre code {
	padding:0;
	font-size:12px;
	background-color:#eee;
	border:none;
}
code {
	font-size:12px;
	background-color:#f8f8ff;
	color:#444;
	padding:0 .2em;
	border:1px solid #dedede;
}
img{border:0;max-width:100%;}
abbr{border-bottom:none;}
a{color:#4183c4;text-decoration:none;}
a:hover{text-decoration:underline;}
a code,a:link code,a:visited code{color:#4183c4;}
h2,h3{margin:1em 0;}
h1,h2,h3,h4,h5,h6{border:0;}
h1{font-size:170%;border-top:4px solid #aaa;padding-top:.5em;margin-top:1.5em;}
h1:first-child{margin-top:0;padding-top:.25em;border-top:none;}
h2{font-size:150%;margin-top:1.5em;border-top:4px solid #e0e0e0;padding-top:.5em;}
h3{margin-top:1em;}
hr{border:1px solid #ddd;}
ul{margin:1em 0 1em 2em;}
ol{margin:1em 0 1em 2em;}
ul li,ol li{margin-top:.5em;margin-bottom:.5em;}
ul ul,ul ol,ol ol,ol ul{margin-top:0;margin-bottom:0;}
blockquote{margin:1em 0;border-left:5px solid #ddd;padding-left:.6em;color:#555;}
dt{font-weight:bold;margin-left:1em;}
dd{margin-left:2em;margin-bottom:1em;}
sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>
<title><big>

CSC 453: Code transformations for GPU race detection with llvm-py
=======

Dong Chen

Introduction</title>

</head>
<body>
<p><big></p>

<h1>CSC 453: Code transformations for GPU race detection with llvm-py</h1>

<p>Dong Chen</p>

<h2>Introduction</h2>

<p>Code transformations are widely used in performance concerned optimizations, programming productivity and so on. LLVM infrastructure is often used. But C++ abased interface is hard to be used during the development. Here we use LLVM-py interface to do code transformations for GPU race detection.</p>

<h2>Architecture and execution model of GPU</h2>

<p>The hardware structure of GPU consists of two major parts: processing part and hierarchical memory part. The processing part is referred as streaming multiprocessors(SM). Each GPU contains a number of SMs and each SM contains an array of streaming processors(SP). The hierarchical memory has three levels: device memory, shared memory and private memory. Device memory can be accessed by all the SPs in every SMs. Each SM has its own shared memory which can be accessed by all the SPs belong to the SM. Each SP has it own private memory which can only be access by itself.</p>

<p>GPU programs contain two parts: the main program and the kernel program. The main program will do initialization, copy data from main memory to device memory, launch kernel program on GPU and copy data back from device memory to main memory once the kernel program finished running. Kernel program will be mapped into thousands of threads. And threads are organized in thread blocks. Thread blocks will be assigned to each SM to execute and threads in the same thread block will be executed on the array of SPs in warps.</p>

<p>So threads in GPU can share data on shared memory and device memory. Data races will occur if synchronization is not correctly used. But current compiler can not detect races.</p>

<p><img src="./4.png" alt="Alt text" /></p>

<h2>GPU Races detection by two-pass run</h2>

<p>Our approach is to detect GPU races by transforming the kernel program and run it on GPU. Two runs are needed: one is to detect write-write races and the other is to detect write-read races. In the first run, we first copy the shared data for each warp and run the threads on their private copies. Then compare the result, if the write regions are not overlapping, it means no write-write races. Else write-write races will happen. The second pass will use the result of the first run as the initial state. Then run and compare, if different, write-read race will happen.</p>

<p><img src="./3.png" alt="Alt text" /></p>

<h2>Code transformations needed</h2>

<ol>
<li><p>iterating through AST and replacing variables</p>

<ul>
<li><p>instructions for declaring new variables</p>

<pre><code>  int a;         ---&gt;     int a[M]
  int a[N];      ---&gt;     int a[M][N]
</code></pre></li>
<li><p>instructions for access redirection</p>

<pre><code>  a              ---&gt;      a[warpID] 
  a[i]           ---&gt;      a[warpID][i]        
</code></pre></li>
</ul>
</li>
<li><p>inserting functions to do memory copy, memory compare</p>

<ul>
<li><pre><code>  region_copy();
  union_copy();
  region_diff();
</code></pre></li>
</ul>
</li>
</ol>


<h2>Update Post 1</h2>

<p>llvmpy can be easily used to generate LLVM IR code, so inserting LLVM IR code of the functions is a good choice. The first step is to generate the functions which will be called by kernels for race detection. Let's take <i>region_copy()</i> as an example, the original C code of <i>region_copy()</i> is listed below, it will copy the data from <i>orig_src</i> to <i>new_copy</i> in parallel:</p>

<pre><code>    void warp_level_parallel_memcpy( int tid,
                   char * dst, char * src, int size)
    {
        int * opt_dst = (int *) dst ;
        int * opt_src = (int *) src ;
        int opt_size  = size / sizeof(int) ;

        int ttid = tid % WARP_SIZE;
        for (int k=0; k&lt; (opt_size / WARP_SIZE)+1; k ++ ) {
            int idx = ttid + WARP_SIZE * k ;
            if ( idx &lt; opt_size )
                opt_dst[idx] = opt_src[idx] ;
        }
        __syncthreads() ;
    }

    void region_copy( int block_id, int tid, char * orig_copy, int size,
                 char * new_copy, char * union_copy)
    {
        warp_level_parallel_memcpy( tid, new_copy, orig_copy, size ) ;
    }
</code></pre>

<p>Writing the above functions in llvmpy is quite straight forward, but should be in the form of LLVM IR. In LLVM IR, functions are contained in modules. Each function should contain at least one "entry" basic block. The instructions are inserted to each basic block of the function in order.</p>

<p>The skeleton is listed below.</p>

<ol>
<li><p>Define data types and function types. llvmpy provides Type object to define the data types and function types.</p>

<pre><code> #define "int" type
 intty = Type.int(32)

 #define "char *" type 
 charty = Type.pointer(Type.int(8))

 #define function type which is "void FUNCTION(int, int, char *, int, char *, char *)"
 fnty = Type.function(Type.void(), [intty, intty, charty, intty, charty, charty])
</code></pre></li>
<li><p>Define a function based on one function type</p>

<pre><code> region_copy = Function.new(mod, fnty, name='region_copy')
</code></pre></li>
<li><p>Setting the arguments. In llvmpy, the arguments of one function can be return just by assignment. And the attributes of each argument can be easily modified.</p>

<pre><code> arg0, arg1, arg2, arg3, arg4, arg5 = region_copy.args
 region_copy.args[0].name = "block_id"
</code></pre></li>
<li><p>Define basic block in one function and inserting instructions</p>

<pre><code> #inserting basic block
 enblk = region_copy.append_basic_block("entry")

 #inserting instructions
 a = bldr.alloca(intty)    # %0 = alloca i32, align 4
 a.alignment = 4
 a_store = bldr.store(arg0, a) # store i32 %block_id, i32* %0

 #inserting instruction for function call
 bldr.call(warp_level_parallel_memcpy,[load_b, load_e, load_c, load_d])
</code></pre></li>
</ol>


<p><b>Summary:</b></p>

<p>What can be done for now(easiest):</p>

<p>All the functions which will be used in the GPU Race detection can be generated. The llvmpy code is listed <a href="https://github.com/dongchen-coder/dongchen-coder.github.io/blob/master/gpuRaceDetection.py">here</a>.
The generated code is listed <a href="https://github.com/dongchen-coder/dongchen-coder.github.io/blob/master/gpuRaceDetection.ll">here</a>.</p>

<p>What should be done next(sort from easy to hard):</p>

<ol>
<li>Find the way to analysis the code, which means find whether there is way to iterating over the code.</li>
<li>Find the way to insert instructions, such as function calls, new variables, and so on.</li>
<li>Find the way to replace variables in instructions.</li>
</ol>


<p>But for now, whether llvmpy supports analysis is still need to be explored. Pass manager part of llvmpy should be further checked.</p>

<h2>Update Post 2 (Draft)</h2>

<ol>
<li><p>iterating over code (LLVM IR), LLVM IR level analysis can be performed</p>

<pre><code> for f in mod.functions:
     for bb in f.basic_blocks:
         for istr in bb.instructions:
             for operand in istr.operands:
</code></pre></li>
<li><p>build compiler yourself</p>

<pre><code> tokenizer
 lexer
 paser (AST can be generated here and )
 code generator
</code></pre></li>
</ol>


<p>Result:</p>

<p>Array access analysis of GPU kernel program:</p>

<p></big></p>
</body>
</html>